{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Do - Spell Correction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lone5267/NLP_Learn/blob/master/Do_Spell_Correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XStW6EFRHnpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#词典库\n",
        "vocab = set([line.rstrip() for line in open('/content/drive/My Drive/Colab Notebooks/DO WORK/SpellCorrectionData/vocab.txt')])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za-HvBKWcba1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#需要生成所有候选集合\n",
        "def generate_candidates(word):\n",
        "    \"\"\"\n",
        "    word: 给定的输入（错误的输入）\n",
        "    返回所有（valid）候选集合\n",
        "    \"\"\"\n",
        "    #生成编辑距离为1的单词\n",
        "    #1.insert 2.delete 3.replace\n",
        "\n",
        "    #假设使用26个字符\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "\n",
        "    #insert操作\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "\n",
        "    #delete操作\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "\n",
        "    #replace操作\n",
        "    replaces = [L + c + R[1:] for L, R in splits for c in letters]\n",
        "    \n",
        "    candidates = set(inserts + deletes + replaces)\n",
        "\n",
        "    #过滤掉不在于词典库里的单词\n",
        "    return [candi for candi in candidates if candi in vocab and candi != word]\n",
        "\n",
        "\n",
        "#generate_candidates(\"apple\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFuDzfyXJjfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "#nltk.download('reuters')\n",
        "#nltk.download('punkt')\n",
        "\n",
        "#读取语料库\n",
        "categories = reuters.categories()\n",
        "corpus = reuters.sents(categories=categories)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmE-JijtOlBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1NJHTPcNocc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#构建语言模型：uigram, bigram, tri-gram, use:bigram\n",
        "\n",
        "term_count = {}\n",
        "bigram_count = {}\n",
        "\n",
        "for doc in corpus:\n",
        "    doc = ['<s>'] + doc\n",
        "    for i in range(0, len(doc) - 1):\n",
        "        #bigram: [i i+1]\n",
        "        term = doc[i]\n",
        "        bigram = doc[i:i+2]\n",
        "\n",
        "        if term in term_count:\n",
        "            term_count[term] += 1\n",
        "        else:\n",
        "            term_count[term] = 1\n",
        "\n",
        "        bigram = ' '.join(bigram)\n",
        "        if bigram in bigram_count:\n",
        "            bigram_count[bigram] += 1\n",
        "        else:\n",
        "            bigram_count[bigram] = 1\n",
        "\n",
        "#print(term_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AEqFS4rhhnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#用户犯错的概率统计 - channel probaility\n",
        "\n",
        "channel_prob = {}\n",
        "for line in open('/content/drive/My Drive/Colab Notebooks/DO WORK/SpellCorrectionData/spell-errors.txt'):\n",
        "    items = line.split(':')\n",
        "    correct = items[0].strip()\n",
        "    mistakes = [item.strip() for item in items[1].strip().split(',')]\n",
        "    channel_prob[correct] = {}\n",
        "    for mis in mistakes:\n",
        "        channel_prob[correct][mis] = 1.0/len(mistakes)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uKeGzqallfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "V = len(term_count.keys())\n",
        "\n",
        "file = open('/content/drive/My Drive/Colab Notebooks/DO WORK/SpellCorrectionData/testdata.txt', 'r')\n",
        "\n",
        "times = 10\n",
        "for line in file:\n",
        "    items = line.rstrip().split('\\t')\n",
        "    lines = items[2].split()\n",
        "    for word in lines:\n",
        "        if word not in vocab:\n",
        "            #需要替换word成正确的单词\n",
        "            #Step1: 生成所有的候选集合\n",
        "            candidates = generate_candidates(word)\n",
        "            probs = []\n",
        "            #对于每个candidate，计算它的score\n",
        "            #score = p(correct)*p(mistake|correct)\n",
        "            #      = log(p(correct)) + log(p(mistake|corrcet))\n",
        "            #返回condidate，会使得score最大\n",
        "            if (len(candidates) < 1):\n",
        "                continue\n",
        "\n",
        "            for candi in candidates:\n",
        "                prob = 0\n",
        "                #a. 计算channel probaility\n",
        "                if candi in channel_prob and word in channel_prob[candi]:\n",
        "                    prob += np.log(channel_prob[candi][word])\n",
        "                else:\n",
        "                    prob += np.log(1.0 / V) #need smoothing\n",
        "                \n",
        "                #b. 计算语言模型的概率\n",
        "                wordList = items[2].split()\n",
        "                idx = wordList.index(word)\n",
        "                \n",
        "                # 计算前项加当前的candi\n",
        "                # TODO 此处教程有误，还要考虑大小写\n",
        "                biword = wordList[idx - 1] + \" \" + candi\n",
        "                if biword in bigram_count and candi in term_count:\n",
        "                    prob += np.log((bigram_count[biword] + 1.0) / (\n",
        "                            term_count[wordList[idx - 1]] + V))\n",
        "                else:\n",
        "                    prob += np.log(1.0 / V)\n",
        "                # TODO: 也要考虑当前 [word, post_word]\n",
        "                #   prob += np.log(bigram概率)\n",
        "                if (idx + 1 < len(wordList)):\n",
        "                    biword = candi + \" \" + wordList[idx + 1]\n",
        "                    if biword in bigram_count and candi in term_count:\n",
        "                        prob += np.log((bigram_count[biword] + 1.0) / (\n",
        "                                term_count[wordList[idx + 1]] + V))\n",
        "                    else:\n",
        "                        prob += np.log(1.0 / V)\n",
        "\n",
        "                probs.append(prob)\n",
        "            #print(probs)\n",
        "            max_idx = probs.index(max(probs))\n",
        "            #print(max_idx)\n",
        "            #print (word, candidates[max_idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW9lUtcj_iZC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}